import asyncio
import os
import random

from amrita_core import ChatObject, init, load_amrita, logger, set_config
from amrita_core.config import AmritaConfig, FunctionConfig, LLMConfig
from amrita_core.preset import PresetManager
from amrita_core.types import MemoryModel, Message, ModelConfig, ModelPreset


def get_random_session_id():
    """Generate a random session ID"""
    return f"session_{random.randint(10000, 99999)}"


def print_welcome_message():
    """Print welcome message"""
    print("=" * 50)
    print("ğŸ¤– Welcome to AmritaCore CLI Chat System")
    print("=" * 50)
    print("Type '/help' to see available commands")
    print("Type '/new' to start a new session")
    print("Type '/quit' to exit the program")
    print("-" * 50)


def print_help():
    """Print help information"""
    help_text = """
ğŸ“– Available Commands:
  Regular text      - Chat with AI assistant
  /new              - Start a new session
  /quit or /exit    - Exit the program
  /help             - Show this help message
  /clear            - Clear screen
  /reset            - Reset conversation history
  /info             - Show current session info
    """
    print(help_text)


async def handle_user_input(
    user_input: str, context: MemoryModel, session_id: str, train: Message
) -> tuple[str, MemoryModel, bool]:
    """Process user input and return new session ID and context"""
    if user_input.lower() in ["/quit", "/exit"]:
        print("ğŸ‘‹ Goodbye! Thank you for using AmritaCore CLI Chat System")
        return session_id, context, False  # Return False to indicate exit

    elif user_input.lower() == "/help":
        print_help()
        return session_id, context, True

    elif user_input.lower() == "/new":
        print("ğŸ”„ Starting a new session...")
        new_session_id = get_random_session_id()
        new_context = MemoryModel()
        print(f"âœ… New session created, ID: {new_session_id}")
        return new_session_id, new_context, True

    elif user_input.lower() == "/clear":
        await asyncio.to_thread(os.system, ("clear" if os.name != "nt" else "cls"))
        print_welcome_message()
        return session_id, context, True

    elif user_input.lower() == "/reset":
        print("ğŸ”„ Resetting conversation history...")
        context = MemoryModel()
        print("âœ… Conversation history reset")
        return session_id, context, True

    elif user_input.lower() == "/info":
        print(f"â„¹ï¸  Current Session: {session_id}")
        print(
            f"ğŸ“Š Conversation Turns: {len(context.messages) if context.messages else 0}"
        )
        return session_id, context, True
    elif user_input.startswith("/"):
        print("âŒ Invalid command")
        return session_id, context, True

    # Process regular user input
    chat = ChatObject(
        context=context,
        session_id=session_id,
        user_input=user_input,
        train=train.model_dump(),
    )

    async def callback():
        nonlocal context
        if chat is None:
            return
        print("ğŸ’¬ Assistant: ", end="")
        async for message in chat.get_response_generator():
            content = message if isinstance(message, str) else message.get_content()
            print(content, end="")
        print("\n")  # New line

    task = asyncio.create_task(callback())

    try:
        await chat.call()
        await task
        # Update context
        context = chat.data
    except Exception as e:
        print(f"âŒ An error occurred: {e!s}")

    return session_id, context, True


async def main():
    print_welcome_message()

    # Load AmritaCore
    # Set configuration
    func = FunctionConfig(
        use_minimal_context=False,
        tool_calling_mode="agent",
        agent_thought_mode="reasoning",
        # agent_mcp_client_enable=True,
        # agent_mcp_server_scripts=[],
    )
    llm = LLMConfig(
        enable_memory_abstract=True,
    )
    config = AmritaConfig(
        function_config=func,
        llm=llm,
    )

    set_config(config)

    await load_amrita()
    os.environ["LOG_LEVEL"] = "WARNING"

    # Add preset model
    preset = ModelPreset(
        model="gpt-3.5-turbo",
        base_url="Insert_your_API_endpoint_here",
        api_key="Insert_your_API_key_here",
        config=ModelConfig(stream=True),
    )

    preset_manager = PresetManager()
    preset_manager.add_preset(preset)
    logger.info("Registered preset.")

    # Initialize session
    context: MemoryModel = MemoryModel()
    session_id = get_random_session_id()
    train = Message(
        content="You are a helpful AI assistant, do not call non-existent tools, follow user's instructions.",
        role="system",
    )

    print(f"âœ¨ Session created, ID: {session_id}")
    print("Start chatting! Type '/help' for help\n")

    # Main loop
    while True:
        try:
            user_input = input("ğŸ‘¤ You: ").strip()

            if not user_input:
                continue

            result_session_id, result_context, continue_flag = await handle_user_input(
                user_input, context, session_id, train
            )

            if not continue_flag:
                break

            # æ›´æ–°session_idå’Œcontext
            session_id = result_session_id
            context = result_context

        except (EOFError, KeyboardInterrupt):
            print("\nğŸ‘‹ Program interrupted, goodbye!")
            break
        except Exception as e:
            print(f"âŒ An unexpected error occurred: {e!s}")
            print("ğŸ’¡ Continuing to run...")


if __name__ == "__main__":
    init()
    asyncio.run(main())
