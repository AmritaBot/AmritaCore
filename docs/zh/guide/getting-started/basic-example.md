# åŸºç¡€ç¤ºä¾‹

## 2.3.1 å®Œæ•´çš„åŸºç¡€åŠŸèƒ½æ¼”ç¤º

è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªæ›´å®Œæ•´çš„ç¤ºä¾‹ï¼Œå±•ç¤ºä¸Šä¸‹æ–‡ä¿ç•™å’Œå¤šæ¬¡äº¤äº’ï¼š

```python
"""
AmritaCore åŸºç¡€ç¤ºä¾‹ - æ ¸å¿ƒåŠŸèƒ½çš„ç®€å•æ¼”ç¤ºã€‚

æ­¤ç¤ºä¾‹æ¼”ç¤ºå¦‚ä½•åˆå§‹åŒ– AmritaCoreã€é…ç½®å®ƒï¼Œ
å¹¶è¿è¡Œä¸ AI åŠ©æ‰‹çš„åŸºæœ¬èŠå¤©ä¼šè¯ã€‚
"""

import asyncio

from amrita_core import ChatObject, init, load_amrita, logger
from amrita_core.config import AmritaConfig, FunctionConfig, LLMConfig
from amrita_core.preset import PresetManager
from amrita_core.types import MemoryModel, Message, ModelConfig, ModelPreset


async def basic_example():
    """
    åŸºç¡€ç¤ºä¾‹ï¼Œæ¼”ç¤º AmritaCore çš„æ ¸å¿ƒåŠŸèƒ½ã€‚
    å±•ç¤ºåˆå§‹åŒ–ã€é…ç½®å’Œç®€å•èŠå¤©äº¤äº’ã€‚
    """
    print("ğŸš€ å¯åŠ¨ AmritaCore åŸºç¡€ç¤ºä¾‹")
    print("-" * 50)

    # é…ç½® AmritaCore
    # FunctionConfig å®šä¹‰Agentçš„ä¸€èˆ¬è¡Œä¸º
    func = FunctionConfig(
        use_minimal_context=False,  # ä½¿ç”¨å®Œæ•´ä¸Šä¸‹æ–‡æˆ–æœ€å°ä¸Šä¸‹æ–‡
        tool_calling_mode="agent",  # å·¥å…·è°ƒç”¨æ–¹å¼
        agent_thought_mode="reasoning",  # Agentè§£å†³é—®é¢˜çš„æ€è€ƒæ–¹å¼
    )

    # LLMConfig å®šä¹‰è¯­è¨€æ¨¡å‹è¡Œä¸º
    llm = LLMConfig(
        enable_memory_abstract=True,  # å¯ç”¨è®°å¿†æ‘˜è¦åŠŸèƒ½
    )

    # å°†é…ç½®åˆå¹¶ä¸ºä¸»é…ç½®
    config = AmritaConfig(
        function_config=func,
        llm=llm,
    )

    # åº”ç”¨é…ç½®
    from amrita_core.config import set_config

    set_config(config)

    # åŠ è½½ AmritaCore ç»„ä»¶
    await load_amrita()

    # è®¾ç½®æ¨¡å‹é¢„è®¾ - å®šä¹‰è¦ä½¿ç”¨çš„ LLM
    preset = ModelPreset(
        model="gpt-3.5-turbo",  # æ¨¡å‹åç§°
        base_url="INSERT_YOUR_API_ENDPOINT_HERE",  # API ç«¯ç‚¹
        api_key="INSERT_YOUR_API_KEY_HERE",  # API å¯†é’¥
        config=ModelConfig(stream=True),  # å¯ç”¨æµå¼å“åº”
    )

    # æ³¨å†Œæ¨¡å‹é¢„è®¾
    preset_manager = PresetManager()
    preset_manager.add_preset(preset)
    preset_manager.set_default_preset(preset.name)
    logger.info("âœ… æ³¨å†Œæ¨¡å‹é¢„è®¾ã€‚")

    # åˆ›å»ºè®°å¿†ä¸Šä¸‹æ–‡ä»¥ä¿å­˜å¯¹è¯å†å²
    context = MemoryModel()

    # å®šä¹‰ç³»ç»ŸæŒ‡ä»¤ï¼ˆAI åº”è¯¥å¦‚ä½•è¡Œä¸ºï¼‰
    train = Message(
        content="æ‚¨æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„ AI åŠ©æ‰‹ã€‚è¯·ç®€æ´å‡†ç¡®åœ°å›ç­”ã€‚",
        role="system",
    )

    print("ğŸ’¬ å¼€å§‹ç¤ºä¾‹å¯¹è¯:")
    print()

    # ç¤ºä¾‹ 1: ç®€å•æ–‡æœ¬äº¤äº’
    user_input = "ä½ å¥½ï¼ä½ èƒ½å‘Šè¯‰æˆ‘ AmritaCore æ˜¯ä»€ä¹ˆå—ï¼Ÿ"

    print(f"ğŸ‘¤ ç”¨æˆ·: {user_input}")

    # åˆ›å»ºç”¨äºäº¤äº’çš„ ChatObject
    chat = ChatObject(
        context=context,
        session_id="basic_example_session",
        user_input=user_input,
        train=train.model_dump(),  # å°† Message è½¬æ¢ä¸ºå­—å…¸
    )

    # å¤„ç†å“åº”å¹¶æ˜¾ç¤º
    print("ğŸ¤– åŠ©æ‰‹: ", end="")

    async with chat.begin():
        async for message in chat.get_response_generator():
            content = message if isinstance(message, str) else message.get_content()
            print(content, end="")

    print("\n")  # å“åº”åæ¢è¡Œ

    # ä½¿ç”¨æœ€æ–°çš„å¯¹è¯çŠ¶æ€æ›´æ–°ä¸Šä¸‹æ–‡
    context = chat.data

    # ç¤ºä¾‹ 2: åç»­é—®é¢˜ä»¥æ¼”ç¤ºä¸Šä¸‹æ–‡ä¿ç•™
    follow_up = "ä½ èƒ½è§£é‡Šå®ƒçš„ä¸»è¦åŠŸèƒ½å—ï¼Ÿ"

    print(f"ğŸ‘¤ ç”¨æˆ·: {follow_up}")

    # ä½¿ç”¨æ›´æ–°çš„ä¸Šä¸‹æ–‡åˆ›å»ºå¦ä¸€ä¸ª ChatObject
    chat2 = ChatObject(
        context=context,
        session_id="basic_example_session",
        user_input=follow_up,
        train=train.model_dump(),
    )

    print("ğŸ¤– åŠ©æ‰‹: ", end="")

    # å¤„ç†åç»­é—®é¢˜
    await chat2.begin()

    async for message in chat2.get_response_generator():
        content = message if isinstance(message, str) else message.get_content()
        print(content, end="")

    print("\n")  # å“åº”åæ¢è¡Œ

    print("ğŸ‰ åŸºç¡€ç¤ºä¾‹æˆåŠŸå®Œæˆï¼")
    print("-" * 50)
    print("ğŸ’¡ æ¼”ç¤ºçš„å…³é”®æ¦‚å¿µ:")
    print("   â€¢ åˆå§‹åŒ–å’Œé…ç½®")
    print("   â€¢ åˆ›å»º ChatObject å®ä¾‹")
    print("   â€¢ æµå¼å“åº”")
    print("   â€¢ ä¸Šä¸‹æ–‡ç®¡ç†")
    print("   â€¢ ä¼šè¯å¤„ç†")


async def minimal_example():
    """
    æœ€å°ç¤ºä¾‹ï¼Œæ˜¾ç¤ºè¿è¡Œ AmritaCore çš„åŸºæœ¬æ­¥éª¤ã€‚
    """
    print("\nğŸ§ª æœ€å°ç¤ºä¾‹")
    print("-" * 30)

    # æœ€å°é…ç½®
    from amrita_core.config import set_config

    set_config(AmritaConfig())

    # åŠ è½½ AmritaCore
    await load_amrita()

    # æ³¨æ„: åœ¨å®é™…åœºæ™¯ä¸­ï¼Œæ‚¨å°†åœ¨æ­¤å¤„é…ç½®æ¨¡å‹é¢„è®¾
    print("âœ… AmritaCore å·²ä½¿ç”¨æœ€å°é…ç½®åŠ è½½")

    # åˆ›å»ºä¸Šä¸‹æ–‡å’Œç³»ç»Ÿæ¶ˆæ¯
    context = MemoryModel()
    train = Message(content="æ‚¨æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚", role="system")

    # åˆ›å»ºå¹¶è¿è¡ŒèŠå¤©äº¤äº’
    chat = ChatObject(
        context=context,
        session_id="minimal_session",
        user_input="ä½ èƒ½åšä»€ä¹ˆï¼Ÿ",
        train=train.model_dump(),
    )


    # æ”¶é›†å“åº”ï¼ˆä»…æ˜¾ç¤ºå…¶å·¥ä½œï¼‰
    async with chat.begin():
        response = await chat.full_response()
    print(f"ğŸ’¬ å“åº”é•¿åº¦: {len(response)} ä¸ªå­—ç¬¦")
    print("âœ… æœ€å°ç¤ºä¾‹å®Œæˆï¼")


if __name__ == "__main__":
    # åˆå§‹åŒ– AmritaCore
    init()

    # è¿è¡Œç¤ºä¾‹
    asyncio.run(basic_example())
    asyncio.run(minimal_example())

    print("\nâœ¨ æ‰€æœ‰ç¤ºä¾‹å·²å®Œæˆï¼")

```

## 2.3.2 é…ç½®è¯¦æƒ…

è¯¥ç¤ºä¾‹å±•ç¤ºäº†å‡ ä¸ªé‡è¦çš„é…ç½®é€‰é¡¹ï¼š

- `use_minimal_context=False`: ä½¿ç”¨å®Œæ•´å¯¹è¯å†å²è€Œä¸æ˜¯ä»…æœ€åä¸€æ¡æ¶ˆæ¯
- `tool_calling_mode="agent"`: é…ç½®å·¥å…·è°ƒç”¨æ–¹å¼
- `agent_thought_mode="reasoning"`: ä½¿Agentåœ¨å“åº”å‰æ€è€ƒé—®é¢˜
- `enable_memory_abstract=True`: å¯ç”¨è‡ªåŠ¨ä¸Šä¸‹æ–‡æ‘˜è¦ä»¥ç®¡ç†Tokenä½¿ç”¨

## 2.3.3 å¸¸è§é—®é¢˜æ’æŸ¥

**é—®é¢˜**: API ç«¯ç‚¹è¿æ¥é”™è¯¯
**è§£å†³æ–¹æ¡ˆ**: éªŒè¯æ‚¨çš„ API ç«¯ç‚¹å’Œå¯†é’¥æ˜¯å¦æ­£ç¡®ï¼Œä»¥åŠç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸ã€‚

**é—®é¢˜**: é«˜Tokenä½¿ç”¨ç‡
**è§£å†³æ–¹æ¡ˆ**: å¯ç”¨è®°å¿†æ‘˜è¦ (`enable_memory_abstract=True`) å¹¶è€ƒè™‘åœ¨ç®€å•æŸ¥è¯¢æ—¶ä½¿ç”¨æœ€å°ä¸Šä¸‹æ–‡æ¨¡å¼ã€‚

**é—®é¢˜**: å“åº”ç¼“æ…¢
**è§£å†³æ–¹æ¡ˆ**: æ£€æŸ¥æ‚¨çš„ç½‘ç»œè¿æ¥å’Œ API æä¾›å•†æ€§èƒ½ã€‚è€ƒè™‘ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ä»¥è·å¾—æ›´å¿«çš„å“åº”ã€‚
